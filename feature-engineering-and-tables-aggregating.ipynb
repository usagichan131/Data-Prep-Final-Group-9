{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.15","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":88498,"databundleVersionId":10139593,"sourceType":"competition"},{"sourceId":10043473,"sourceType":"datasetVersion","datasetId":6187199},{"sourceId":10054611,"sourceType":"datasetVersion","datasetId":6195401},{"sourceId":10083690,"sourceType":"datasetVersion","datasetId":6216794},{"sourceId":10083990,"sourceType":"datasetVersion","datasetId":6217004}],"dockerImageVersionId":30786,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nfrom IPython.display import display\nimport pandas as pd\nimport numpy as np\nimport sklearn\nimport matplotlib.pyplot as plt\nimport os\nimport seaborn as sns\nimport multiprocessing as mp\n\npd.set_option(\"display.max_columns\", 100)\npd.set_option(\"display.max_rows\", 100)\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2024-12-04T11:25:30.660798Z","iopub.execute_input":"2024-12-04T11:25:30.661219Z","iopub.status.idle":"2024-12-04T11:25:35.020669Z","shell.execute_reply.started":"2024-12-04T11:25:30.661170Z","shell.execute_reply":"2024-12-04T11:25:35.019991Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nimport gc","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-04T11:25:36.300042Z","iopub.execute_input":"2024-12-04T11:25:36.300683Z","iopub.status.idle":"2024-12-04T11:25:36.304052Z","shell.execute_reply.started":"2024-12-04T11:25:36.300647Z","shell.execute_reply":"2024-12-04T11:25:36.303374Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# TRANSFORM 4 TABLES: CREDIT_CARD_BALANCE, INSTALLMENTS_PAYMENTS, POS_CASH and PREVIOUS_APPLICATION","metadata":{}},{"cell_type":"markdown","source":"In this section, we encode and create new features for these four tables.","metadata":{}},{"cell_type":"code","source":"from scipy.stats import kurtosis, iqr, skew\n\n\ndef add_features_in_group(features, gr_, feature_name, aggs, prefix):\n    '''\n    Add new features to a dataframe groupby object.\n        Input:\n            features : dict\n                Dictionary to store new features.\n            gr_ : pandas.DataFrame\n                Dataframe groupby object.\n            feature_name : str\n                Column name to calculate statistics.\n            aggs : list\n                List of method to calculate statistics.\n            prefix : str\n                Prefix of column names.\n        Output:\n            features : dict\n                Dictionary with new features.\n        '''\n    for agg in aggs:\n        if agg == 'sum':\n            features[f'{prefix}{feature_name}_sum'] = gr_[\n                feature_name].sum()\n        elif agg == 'mean':\n            features[f'{prefix}{feature_name}_mean'] = gr_[\n                feature_name].mean()\n        elif agg == 'max':\n            features[f'{prefix}{feature_name}_max'] = gr_[\n                feature_name].max()\n        elif agg == 'min':\n            features[f'{prefix}{feature_name}_min'] = gr_[\n                feature_name].min()\n        elif agg == 'std':\n            features[f'{prefix}{feature_name}_std'] = gr_[\n                feature_name].std()\n        elif agg == 'count':\n            features[f'{prefix}{feature_name}_count'] = gr_[\n                feature_name].count()\n        elif agg == 'skew':\n            features[f'{prefix}{feature_name}_skew'] = skew(\n                gr_[feature_name])\n        elif agg == 'kurt':\n            features[f'{prefix}{feature_name}_kurt'] = kurtosis(\n                gr_[feature_name])\n        elif agg == 'iqr':\n            features[f'{prefix}{feature_name}_iqr'] = iqr(\n                gr_[feature_name])\n        elif agg == 'median':\n            features[f'{prefix}{feature_name}_median'] = gr_[\n                feature_name].median()\n    return features\n\n\ndef installments_last_loan_features(gr):\n    '''\n    Calculate features for the last loan in installments_payments.csv.\n        Input:\n            gr : pandas.DataFrame\n                DataFrame groupby object.\n        Output:\n            features : dict\n                Dictionary with new features.\n    '''\n    gr_ = gr.copy()\n    gr_.sort_values(['DAYS_INSTALMENT'], ascending=False, inplace=True)\n    last_installment_id = gr_['SK_ID_PREV'].iloc[0]\n    gr_ = gr_[gr_['SK_ID_PREV'] == last_installment_id]\n\n    features = {}\n    features = add_features_in_group(features, gr_, 'DPD',\n                                     ['sum', 'mean', 'max', 'std'],\n                                     'LAST_LOAN_')\n    features = add_features_in_group(features, gr_, 'LATE_PAYMENT',\n                                     ['count', 'mean'],\n                                     'LAST_LOAN_')\n    features = add_features_in_group(features, gr_, 'PAID_OVER_AMOUNT',\n                                     ['sum', 'mean', 'max', 'min', 'std'],\n                                     'LAST_LOAN_')\n    features = add_features_in_group(features, gr_, 'PAID_OVER',\n                                     ['count', 'mean'],\n                                     'LAST_LOAN_')\n    return features\n\n\ndef add_ratios_features(df):\n    '''\n    Calculate several ratios for the main dataset.\n        Input:\n            df : pandas.DataFrame\n                Dataframe after merge with all other dataframes.\n        Output:\n            df : pandas.DataFrame\n                Final ataframe with ratios added.\n    '''\n\n    # CREDIT TO INCOME RATIO\n    df['BUREAU_INCOME_CREDIT_RATIO'] = df['BUREAU_AMT_CREDIT_SUM_MEAN'] / \\\n        df['AMT_INCOME_TOTAL']\n    df['BUREAU_ACTIVE_CREDIT_TO_INCOME_RATIO'] = df['BUREAU_ACTIVE_AMT_CREDIT_SUM_SUM'] / \\\n        df['AMT_INCOME_TOTAL']\n\n    # PREVIOUS TO CURRENT CREDIT RATIO\n    df['CURRENT_TO_APPROVED_CREDIT_MIN_RATIO'] = df['APPROVED_AMT_CREDIT_MIN'] / \\\n        df['AMT_CREDIT']\n    df['CURRENT_TO_APPROVED_CREDIT_MAX_RATIO'] = df['APPROVED_AMT_CREDIT_MAX'] / \\\n        df['AMT_CREDIT']\n    df['CURRENT_TO_APPROVED_CREDIT_MEAN_RATIO'] = df['APPROVED_AMT_CREDIT_MEAN'] / df['AMT_CREDIT']\n\n    # PREVIOUS TO CURRENT ANNUITY RATIO\n    df['CURRENT_TO_APPROVED_ANNUITY_MAX_RATIO'] = df['APPROVED_AMT_ANNUITY_MAX'] / \\\n        df['AMT_ANNUITY']\n    df['CURRENT_TO_APPROVED_ANNUITY_MEAN_RATIO'] = df['APPROVED_AMT_ANNUITY_MEAN'] / \\\n        df['AMT_ANNUITY']\n    df['PAYMENT_MIN_TO_ANNUITY_RATIO'] = df['INS_AMT_PAYMENT_MIN'] / df['AMT_ANNUITY']\n    df['PAYMENT_MAX_TO_ANNUITY_RATIO'] = df['INS_AMT_PAYMENT_MAX'] / df['AMT_ANNUITY']\n    df['PAYMENT_MEAN_TO_ANNUITY_RATIO'] = df['INS_AMT_PAYMENT_MEAN'] / df['AMT_ANNUITY']\n\n    # PREVIOUS TO CURRENT CREDIT TO ANNUITY RATIO\n    df['CTA_CREDIT_TO_ANNUITY_MAX_RATIO'] = df['APPROVED_CREDIT_TO_ANNUITY_RATIO_MAX'] / \\\n        df['CREDIT_TO_ANNUITY_RATIO']\n    df['CTA_CREDIT_TO_ANNUITY_MEAN_RATIO'] = df['APPROVED_CREDIT_TO_ANNUITY_RATIO_MEAN'] / \\\n        df['CREDIT_TO_ANNUITY_RATIO']\n\n    # DAYS DIFFERENCES AND RATIOS\n    df['DAYS_DECISION_MEAN_TO_BIRTH'] = df['APPROVED_DAYS_DECISION_MEAN'] / \\\n        df['DAYS_BIRTH']\n    df['DAYS_CREDIT_MEAN_TO_BIRTH'] = df['BUREAU_DAYS_CREDIT_MEAN'] / df['DAYS_BIRTH']\n    df['DAYS_DECISION_MEAN_TO_EMPLOYED'] = df['APPROVED_DAYS_DECISION_MEAN'] / \\\n        df['DAYS_EMPLOYED']\n    df['DAYS_CREDIT_MEAN_TO_EMPLOYED'] = df['BUREAU_DAYS_CREDIT_MEAN'] / \\\n        df['DAYS_EMPLOYED']\n    return df\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-04T11:25:42.398491Z","iopub.execute_input":"2024-12-04T11:25:42.399128Z","iopub.status.idle":"2024-12-04T11:25:42.413236Z","shell.execute_reply.started":"2024-12-04T11:25:42.399086Z","shell.execute_reply":"2024-12-04T11:25:42.412462Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def one_hot_encoder(df, categorical_columns=None, nan_as_category=True):\n    \"\"\"Create a new column for each categorical value in categorical columns using get dummies. \"\"\"\n    original_columns = list(df.columns)\n    if not categorical_columns:\n        categorical_columns = [\n            col for col in df.columns if df[col].dtype == 'object']\n    df = pd.get_dummies(df, columns=categorical_columns,\n                        dummy_na=nan_as_category)\n    categorical_columns = [c for c in df.columns if c not in original_columns]\n    return df, categorical_columns\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-04T11:25:50.534272Z","iopub.execute_input":"2024-12-04T11:25:50.535124Z","iopub.status.idle":"2024-12-04T11:25:50.539674Z","shell.execute_reply.started":"2024-12-04T11:25:50.535089Z","shell.execute_reply":"2024-12-04T11:25:50.538949Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def group(df_to_agg, prefix, aggregations, aggregate_by='SK_ID_CURR'):\n    agg_df = df_to_agg.groupby(aggregate_by).agg(aggregations)\n    agg_df.columns = pd.Index(['{}{}_{}'.format(prefix, e[0], e[1].upper())\n                               for e in agg_df.columns.tolist()])\n    return agg_df.reset_index()\n\n\ndef group_and_merge(df_to_agg, df_to_merge, prefix, aggregations, aggregate_by='SK_ID_CURR'):\n    agg_df = group(df_to_agg, prefix, aggregations, aggregate_by=aggregate_by)\n    return df_to_merge.merge(agg_df, how='left', on=aggregate_by)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-04T11:25:54.678509Z","iopub.execute_input":"2024-12-04T11:25:54.678849Z","iopub.status.idle":"2024-12-04T11:25:54.755138Z","shell.execute_reply.started":"2024-12-04T11:25:54.678819Z","shell.execute_reply":"2024-12-04T11:25:54.754283Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\nPREVIOUS_AGG = {\n    'SK_ID_PREV': ['nunique'],\n\n    'AMT_ANNUITY': ['min', 'max', 'mean'],\n    'AMT_DOWN_PAYMENT': ['max', 'mean'],\n    'APPLICATION_CREDIT_DIFF': ['min', 'max', 'mean'],\n    'APPLICATION_CREDIT_RATIO': ['min', 'max', 'mean', 'var'],\n\n    'CNT_PAYMENT': ['max', 'mean'],\n    'CREDIT_TO_ANNUITY_RATIO': ['mean', 'max'],\n\n    'DAYS_TERMINATION': ['max'],\n    'DAYS_DECISION': ['min', 'max', 'mean'],\n    'DOWN_PAYMENT_TO_CREDIT': ['mean'],\n\n    'HOUR_APPR_PROCESS_START': ['min', 'max', 'mean'],\n\n    'NEW_CREDIT_GOODS_RATE': ['mean', 'max', 'var', 'min'],\n    'NEW_END_DIFF': ['min', 'max'],\n    'NEW_DAYS_DUE_DIFF': ['min', 'max'],\n    'NEW_RETURN_DAY': ['min', 'max'],\n\n    'RATE_DOWN_PAYMENT': ['max', 'mean'],\n}\n\nPREVIOUS_ACTIVE_AGG = {\n    'SK_ID_PREV': ['nunique'],\n\n    'AMT_ANNUITY': ['max', 'sum'],\n    'AMT_APPLICATION': ['max', 'mean'],\n    'AMT_CREDIT': ['sum'],\n    'AMT_DOWN_PAYMENT': ['max', 'mean'],\n    'AMT_PAYMENT': ['sum'],\n\n    'CNT_PAYMENT': ['mean', 'sum'],\n\n    'DAYS_DECISION': ['min', 'mean'],\n    'DAYS_LAST_DUE_1ST_VERSION': ['min', 'max', 'mean'],\n\n    'INSTALMENT_PAYMENT_DIFF': ['mean', 'max'],\n\n    'REMAINING_DEBT': ['max', 'mean', 'sum'],\n    'REPAYMENT_RATIO': ['mean'],\n\n    'SIMPLE_INTERESTS': ['mean'],\n}\n\nPREVIOUS_APPROVED_AGG = {\n    'SK_ID_PREV': ['nunique'],\n\n    'AMT_ANNUITY': ['min', 'max', 'mean'],\n    'AMT_CREDIT': ['min', 'max', 'mean'],\n    'AMT_DOWN_PAYMENT': ['max'],\n    'AMT_GOODS_PRICE': ['max'],\n    'AMT_INTEREST': ['min', 'max', 'mean'],\n    'APPLICATION_CREDIT_DIFF': ['max'],\n    'APPLICATION_CREDIT_RATIO': ['min', 'max', 'mean'],\n\n    'CNT_PAYMENT': ['max', 'mean'],\n    'CREDIT_TO_ANNUITY_RATIO': ['mean', 'max'],\n\n    'DAYS_DECISION': ['min', 'mean'],\n    'DAYS_TERMINATION': ['mean'],\n    'DAYS_FIRST_DRAWING': ['max', 'mean'],\n    'DAYS_FIRST_DUE': ['min', 'mean'],\n    'DAYS_LAST_DUE_1ST_VERSION': ['min', 'max', 'mean'],\n    'DAYS_LAST_DUE': ['max', 'mean'],\n    'DAYS_LAST_DUE_DIFF': ['min', 'max', 'mean'],\n\n    'HOUR_APPR_PROCESS_START': ['min', 'max'],\n\n    'INTEREST_SHARE': ['min', 'max', 'mean'],\n    'INTEREST_RATE': ['min', 'max', 'mean'],\n\n    'SIMPLE_INTERESTS': ['min', 'max', 'mean'],\n}\n\nPREVIOUS_REFUSED_AGG = {\n    'AMT_APPLICATION': ['max', 'mean'],\n    'AMT_CREDIT': ['min', 'max'],\n    'APPLICATION_CREDIT_DIFF': ['min', 'max', 'mean', 'var'],\n    'APPLICATION_CREDIT_RATIO': ['min', 'mean'],\n\n    'CNT_PAYMENT': ['max', 'mean'],\n\n    'DAYS_DECISION': ['min', 'max', 'mean'],\n\n    'NAME_CONTRACT_TYPE_Consumer loans': ['mean'],\n    'NAME_CONTRACT_TYPE_Cash loans': ['mean'],\n    'NAME_CONTRACT_TYPE_Revolving loans': ['mean'],\n}\n\nPREVIOUS_LATE_PAYMENTS_AGG = {\n    'APPLICATION_CREDIT_DIFF': ['min'],\n\n    'DAYS_DECISION': ['min', 'max', 'mean'],\n    'DAYS_LAST_DUE_1ST_VERSION': ['min', 'max', 'mean'],\n\n    'NAME_CONTRACT_TYPE_Consumer loans': ['mean'],\n    'NAME_CONTRACT_TYPE_Cash loans': ['mean'],\n    'NAME_CONTRACT_TYPE_Revolving loans': ['mean'],\n}\n\nPREVIOUS_LOAN_TYPE_AGG = {\n    'AMT_CREDIT': ['sum'],\n    'AMT_ANNUITY': ['mean', 'max'],\n    'APPLICATION_CREDIT_DIFF': ['min', 'var'],\n    'APPLICATION_CREDIT_RATIO': ['min', 'max', 'mean'],\n\n    'DAYS_DECISION': ['max'],\n    'DAYS_LAST_DUE_1ST_VERSION': ['max', 'mean'],\n\n    'SIMPLE_INTERESTS': ['min', 'mean', 'max', 'var'],\n}\nPREVIOUS_TIME_AGG = {\n    'AMT_CREDIT': ['sum'],\n    'AMT_ANNUITY': ['mean', 'max'],\n    'APPLICATION_CREDIT_DIFF': ['min'],\n    'APPLICATION_CREDIT_RATIO': ['min', 'max', 'mean'],\n\n    'DAYS_DECISION': ['min', 'mean'],\n    'DAYS_LAST_DUE_1ST_VERSION': ['min', 'max', 'mean'],\n\n    'NAME_CONTRACT_TYPE_Consumer loans': ['mean'],\n    'NAME_CONTRACT_TYPE_Cash loans': ['mean'],\n    'NAME_CONTRACT_TYPE_Revolving loans': ['mean'],\n\n    'SIMPLE_INTERESTS': ['mean', 'max'],\n}\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## PREVIOUS_APPLICATION","metadata":{}},{"cell_type":"code","source":"def previous_application(path_to_data):\n    \"\"\" Process mainly on dseb63_previous_application.csv and and merge with \n    some solumns of dseb63_installments_payments.csv for insights return a pandas dataframe. \"\"\"\n    # Read data dseb63_previous_application.csv and dseb63_installments_payments.csv\n    prev = pd.read_csv(os.path.join(\n        path_to_data, 'dseb63_previous_application.csv'))\n    pay = pd.read_csv(os.path.join(\n        path_to_data, 'dseb63_installments_payments.csv'))\n\n    # One-hot encode most important categorical features\n    enc_columns = [\n        'NAME_CONTRACT_STATUS', 'NAME_CONTRACT_TYPE', 'CHANNEL_TYPE',\n        'NAME_TYPE_SUITE', 'NAME_YIELD_GROUP', 'PRODUCT_COMBINATION',\n        'NAME_PRODUCT_TYPE', 'NAME_CLIENT_TYPE']\n    prev, categorical_cols = one_hot_encoder(\n        prev, enc_columns, nan_as_category=False)\n\n    new_coding = {\"0\": \"Yes\", \"1\": \"No\"}\n    # Calculate ratios and difference for some columns\n    prev['APPLICATION_CREDIT_DIFF'] = prev['AMT_APPLICATION'] - prev['AMT_CREDIT']\n    prev['APPLICATION_CREDIT_RATIO'] = prev['AMT_APPLICATION'] / prev['AMT_CREDIT']\n\n    prev['CREDIT_TO_ANNUITY_RATIO'] = prev['AMT_CREDIT']/prev['AMT_ANNUITY']\n\n    prev['DOWN_PAYMENT_TO_CREDIT'] = prev['AMT_DOWN_PAYMENT'] / prev['AMT_CREDIT']\n    prev[\"NEW_APP_CREDIT_RATE_RATIO\"] = prev[\"APPLICATION_CREDIT_RATIO\"].apply(\n        lambda x: 1 if (x <= 1) else 0)\n    prev['NEW_APP_CREDIT_RATE_RATIO'] = prev['NEW_APP_CREDIT_RATE_RATIO'].astype(\n        'O')\n    prev['NEW_APP_CREDIT_RATE_RATIO'] = prev['NEW_APP_CREDIT_RATE_RATIO'].replace(\n        new_coding)\n\n    prev[\"NEW_CNT_PAYMENT\"] = pd.cut(x=prev['CNT_PAYMENT'], bins=[\n                                     0, 12, 60, 120], labels=[\"Short\", \"Middle\", \"Long\"])\n    prev['NEW_CREDIT_GOODS_RATE'] = prev['AMT_CREDIT'] / prev['AMT_GOODS_PRICE']\n\n    prev[\"NEW_END_DIFF\"] = prev[\"DAYS_TERMINATION\"] - prev[\"DAYS_LAST_DUE\"]\n    prev['NEW_DAYS_DUE_DIFF'] = prev['DAYS_LAST_DUE_1ST_VERSION'] - \\\n        prev['DAYS_FIRST_DUE']\n    prev['NEW_RETURN_DAY'] = prev['DAYS_DECISION'] + prev['CNT_PAYMENT'] * 30\n    prev['NEW_DAYS_TERMINATION_DIFF'] = prev['DAYS_TERMINATION'] - \\\n        prev['NEW_RETURN_DAY']\n\n    prev['NFLAG_LAST_APPL_IN_DAY'] = prev['NFLAG_LAST_APPL_IN_DAY'].astype(\"O\")\n\n    # Interest ratio on previous application\n    total_payment = prev['AMT_ANNUITY'] * prev['CNT_PAYMENT']\n    prev['SIMPLE_INTERESTS'] = (\n        total_payment/prev['AMT_CREDIT'] - 1)/prev['CNT_PAYMENT']\n    prev['AMT_INTEREST'] = prev['CNT_PAYMENT'] * \\\n        prev['AMT_ANNUITY'] - prev['AMT_CREDIT']\n    prev['INTEREST_SHARE'] = prev['AMT_INTEREST'] / \\\n        (prev['AMT_CREDIT'] + 0.00001)  # smoothing to avoid division by zero\n    prev['INTEREST_RATE'] = 2 * 12 * prev['AMT_INTEREST'] / \\\n        (prev['AMT_CREDIT'] * (prev['CNT_PAYMENT'] + 1))\n\n    # Active loans - approved and not complete yet (last_due 365243)\n    approved = prev[prev['NAME_CONTRACT_STATUS_Approved'] == 1]\n    active_df = approved[approved['DAYS_LAST_DUE'] == 365243]\n\n    # Find how much was already payed in active loans (using installments csv)\n    active_pay = pay[pay['SK_ID_PREV'].isin(active_df['SK_ID_PREV'])]\n    active_pay_agg = active_pay.groupby(\n        'SK_ID_PREV')[['AMT_INSTALMENT', 'AMT_PAYMENT']].sum()\n    active_pay_agg.reset_index(inplace=True)\n\n    # Active loans: difference of what was payed and installments\n    active_pay_agg['INSTALMENT_PAYMENT_DIFF'] = active_pay_agg['AMT_INSTALMENT'] - \\\n        active_pay_agg['AMT_PAYMENT']\n\n    # Merge with active_df\n    active_df = active_df.merge(active_pay_agg, on='SK_ID_PREV', how='left')\n    active_df['REMAINING_DEBT'] = active_df['AMT_CREDIT'] - \\\n        active_df['AMT_PAYMENT']\n    active_df['REPAYMENT_RATIO'] = active_df['AMT_PAYMENT'] / \\\n        active_df['AMT_CREDIT']\n\n    # Perform aggregations for active applications\n    active_agg_df = group(active_df, 'PREV_ACTIVE_', PREVIOUS_ACTIVE_AGG)\n    active_agg_df['TOTAL_REPAYMENT_RATIO'] = active_agg_df['PREV_ACTIVE_AMT_PAYMENT_SUM'] /\\\n        active_agg_df['PREV_ACTIVE_AMT_CREDIT_SUM']\n    del active_pay, active_pay_agg, active_df\n\n    # Change 365.243 values to nan (missing)\n    prev['DAYS_FIRST_DRAWING'].replace(365243, np.nan, inplace=True)\n    prev['DAYS_FIRST_DUE'].replace(365243, np.nan, inplace=True)\n    prev['DAYS_LAST_DUE_1ST_VERSION'].replace(365243, np.nan, inplace=True)\n    prev['DAYS_LAST_DUE'].replace(365243, np.nan, inplace=True)\n    prev['DAYS_TERMINATION'].replace(365243, np.nan, inplace=True)\n\n    # Days last due difference (scheduled x done)\n    prev['DAYS_LAST_DUE_DIFF'] = prev['DAYS_LAST_DUE_1ST_VERSION'] - \\\n        prev['DAYS_LAST_DUE']\n    approved['DAYS_LAST_DUE_DIFF'] = approved['DAYS_LAST_DUE_1ST_VERSION'] - \\\n        approved['DAYS_LAST_DUE']\n\n    # Categorical features\n    categorical_agg = {key: ['mean'] for key in categorical_cols}\n\n    # Perform general aggregations\n    agg_prev = group(prev, 'PREV_', {**PREVIOUS_AGG, **categorical_agg})\n\n    # Merge active loans dataframe on agg_prev\n    agg_prev = agg_prev.merge(active_agg_df, how='left', on='SK_ID_CURR')\n    del active_agg_df\n\n    # Aggregations for approved and refused loans\n    agg_prev = group_and_merge(\n        approved, agg_prev, 'APPROVED_', PREVIOUS_APPROVED_AGG)\n    refused = prev[prev['NAME_CONTRACT_STATUS_Refused'] == 1]\n    agg_prev = group_and_merge(\n        refused, agg_prev, 'REFUSED_', PREVIOUS_REFUSED_AGG)\n    del approved, refused\n\n    # Aggregations for Consumer loans and Cash loans\n    for loan_type in ['Consumer loans', 'Cash loans']:\n        type_df = prev[prev[f'NAME_CONTRACT_TYPE_{loan_type}'] == 1]\n        prefix = 'PREV_' + loan_type.split(\" \", maxsplit=1)[0] + '_'\n        agg_prev = group_and_merge(\n            type_df, agg_prev, prefix, PREVIOUS_LOAN_TYPE_AGG)\n    del type_df\n\n    # Get the SK_ID_PREV for loans with late payments (days past due)\n    pay['LATE_PAYMENT'] = pay['DAYS_ENTRY_PAYMENT'] - pay['DAYS_INSTALMENT']\n    pay['LATE_PAYMENT'] = pay['LATE_PAYMENT'].apply(\n        lambda x: 1 if x > 0 else 0)\n    dpd_id = pay[pay['LATE_PAYMENT'] > 0]['SK_ID_PREV'].unique()\n\n    # Aggregations for loans with late payments\n    agg_dpd = group_and_merge(prev[prev['SK_ID_PREV'].isin(dpd_id)], agg_prev,\n                              'PREV_LATE_', PREVIOUS_LATE_PAYMENTS_AGG)\n    del agg_dpd, dpd_id\n\n    # Aggregations for loans in the last x months\n    for time_frame in [12, 24]:\n        time_frame_df = prev[prev['DAYS_DECISION'] >= -30*time_frame]\n        prefix = f'PREV_LAST{time_frame}M_'\n        agg_prev = group_and_merge(\n            time_frame_df, agg_prev, prefix, PREVIOUS_TIME_AGG)\n        del time_frame_df\n\n    del prev\n    gc.collect()\n    return agg_prev\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-04T11:25:58.912361Z","iopub.execute_input":"2024-12-04T11:25:58.912988Z","iopub.status.idle":"2024-12-04T11:25:58.930673Z","shell.execute_reply.started":"2024-12-04T11:25:58.912953Z","shell.execute_reply":"2024-12-04T11:25:58.930058Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"path = '/kaggle/input/dseb-64-data-preparation-final-project/dseb63_final_project_DP_dataset/dseb63_final_project_DP_dataset/'","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-04T11:26:08.777651Z","iopub.execute_input":"2024-12-04T11:26:08.778192Z","iopub.status.idle":"2024-12-04T11:26:08.781339Z","shell.execute_reply.started":"2024-12-04T11:26:08.778159Z","shell.execute_reply":"2024-12-04T11:26:08.780745Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"prev = previous_application(path)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-04T11:26:11.335626Z","iopub.execute_input":"2024-12-04T11:26:11.335989Z","iopub.status.idle":"2024-12-04T11:26:44.050851Z","shell.execute_reply.started":"2024-12-04T11:26:11.335957Z","shell.execute_reply":"2024-12-04T11:26:44.049907Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def do_sum(dataframe, group_cols, counted, agg_name):\n    gp = dataframe[group_cols + [counted]].groupby(\n        group_cols)[counted].sum().reset_index().rename(columns={counted: agg_name})\n    dataframe = dataframe.merge(gp, on=group_cols, how='left')\n    return dataframe","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-04T11:26:44.052406Z","iopub.execute_input":"2024-12-04T11:26:44.052672Z","iopub.status.idle":"2024-12-04T11:26:44.057345Z","shell.execute_reply.started":"2024-12-04T11:26:44.052647Z","shell.execute_reply":"2024-12-04T11:26:44.056514Z"},"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## POS_CASH","metadata":{}},{"cell_type":"code","source":"POS_CASH_AGG = {\n    'SK_ID_PREV': ['nunique'],\n    'SK_ID_CURR': ['count'],\n\n    'EXP_CNT_INSTALMENT': ['last', 'min', 'max', 'mean', 'sum'],\n    'EXP_CNT_INSTALMENT_FUTURE': ['last', 'min', 'max', 'mean', 'sum'],\n\n    'LATE_PAYMENT': ['mean'],\n\n    'MONTHS_BALANCE': ['min', 'max', 'size'],\n\n    'POS_IS_DPD': ['mean', 'sum'],\n    'POS_IS_DPD_UNDER_120': ['mean', 'sum'],\n    'POS_IS_DPD_OVER_120': ['mean', 'sum'],\n\n    'SK_DPD': ['max', 'mean', 'sum', 'var', 'min'],\n    'SK_DPD_DEF': ['max', 'mean', 'sum'],\n}","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-04T11:26:44.058350Z","iopub.execute_input":"2024-12-04T11:26:44.058605Z","iopub.status.idle":"2024-12-04T11:26:44.072690Z","shell.execute_reply.started":"2024-12-04T11:26:44.058579Z","shell.execute_reply":"2024-12-04T11:26:44.071987Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def pos_cash(path_to_data):\n    \"\"\" Process dseb63_POS_CASH_balance.csv and return a pandas dataframe. \"\"\"\n    pos = pd.read_csv(os.path.join(\n        path_to_data, 'dseb63_POS_CASH_balance.csv'))\n\n    # computing Exponential Moving Average for some features based on MONTHS_BALANCE\n    columns_for_ema = ['CNT_INSTALMENT', 'CNT_INSTALMENT_FUTURE']\n    exp_columns = ['EXP_'+ele for ele in columns_for_ema]\n    pos[exp_columns] = pos.groupby('SK_ID_PREV')[columns_for_ema].transform(\n        lambda x: x.ewm(alpha=0.6).mean())\n\n    # One-hot encode categorical features\n    pos, categorical_cols = one_hot_encoder(pos, nan_as_category=False)\n\n    # Flag months with late payment\n    pos['LATE_PAYMENT'] = pos['SK_DPD'].apply(lambda x: 1 if x > 0 else 0)\n\n    # Flag days past due\n    pos['POS_IS_DPD'] = pos['SK_DPD'].apply(lambda x: 1 if x > 0 else 0)\n\n    # Flag days past due less than 120 days\n    pos['POS_IS_DPD_UNDER_120'] = pos['SK_DPD'].apply(\n        lambda x: 1 if (x > 0) & (x < 120) else 0)\n\n    # Flag days past due over 120 days\n    pos['POS_IS_DPD_OVER_120'] = pos['SK_DPD'].apply(\n        lambda x: 1 if x >= 120 else 0)\n\n    # Aggregate by SK_ID_CURR\n    categorical_agg = {key: ['mean'] for key in categorical_cols}\n    pos_agg = group(pos, 'POS_', {**POS_CASH_AGG, **categorical_agg})\n\n    # Sort and group by SK_ID_PREV\n    sort_pos = pos.sort_values(by=['SK_ID_PREV', 'MONTHS_BALANCE'])\n    gp = sort_pos.groupby('SK_ID_PREV')\n\n    # Create new dataframe to store features calculated from gp\n    df = pd.DataFrame()\n\n    df['SK_ID_CURR'] = gp['SK_ID_CURR'].first()\n    df['MONTHS_BALANCE_MAX'] = gp['MONTHS_BALANCE'].max()\n\n    # Percentage of previous loans completed and completed before initial term\n    df['POS_LOAN_COMPLETED_MEAN'] = gp['NAME_CONTRACT_STATUS_Completed'].mean()\n    df['POS_COMPLETED_BEFORE_MEAN'] = gp['CNT_INSTALMENT'].first() - \\\n        gp['CNT_INSTALMENT'].last()\n    df['POS_COMPLETED_BEFORE_MEAN'] = df.apply(lambda x: 1 if x['POS_COMPLETED_BEFORE_MEAN'] > 0\n                                               and x['POS_LOAN_COMPLETED_MEAN'] > 0 else 0, axis=1)\n\n    # Number of remaining installments (future installments) and percentage from total\n    df['POS_REMAINING_INSTALMENTS'] = gp['CNT_INSTALMENT_FUTURE'].last()\n    df['POS_REMAINING_INSTALMENTS_RATIO'] = gp['CNT_INSTALMENT_FUTURE'].last() / \\\n        gp['CNT_INSTALMENT'].last()\n\n    # Group by SK_ID_CURR and merge\n    df_gp = df.groupby('SK_ID_CURR').sum().reset_index()\n    df_gp.drop(['MONTHS_BALANCE_MAX'], axis=1, inplace=True)\n    pos_agg = pd.merge(pos_agg, df_gp, on='SK_ID_CURR', how='left')\n    del df, gp, df_gp, sort_pos\n\n    # Percentage of late payments for the 3 most recent applications\n    pos = do_sum(pos, ['SK_ID_PREV'], 'LATE_PAYMENT', 'LATE_PAYMENT_SUM')\n\n    # Last month of each application\n    last_month_df = pos.groupby('SK_ID_PREV')['MONTHS_BALANCE'].idxmax()\n\n    # Most recent applications (last 3)\n    sort_pos = pos.sort_values(by=['SK_ID_PREV', 'MONTHS_BALANCE'])\n    gp = sort_pos.iloc[last_month_df].groupby('SK_ID_CURR').tail(3)\n\n    # Average application features over the last 3 applications\n    gp_mean = gp.groupby('SK_ID_CURR').mean().reset_index()\n    pos_agg = pd.merge(\n        pos_agg, gp_mean[['SK_ID_CURR', 'LATE_PAYMENT_SUM']], on='SK_ID_CURR', how='left')\n\n    # Drop some useless categorical features, which were created to calculate to other features\n    drop_features = [\n        'POS_NAME_CONTRACT_STATUS_Canceled_MEAN', 'POS_NAME_CONTRACT_STATUS_Amortized debt_MEAN',\n        'POS_NAME_CONTRACT_STATUS_XNA_MEAN']\n    pos_agg.drop(drop_features, axis=1, inplace=True)\n    del gp, gp_mean, sort_pos, pos\n    gc.collect()\n    return pos_agg\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-04T11:26:44.074375Z","iopub.execute_input":"2024-12-04T11:26:44.074609Z","iopub.status.idle":"2024-12-04T11:26:44.087089Z","shell.execute_reply.started":"2024-12-04T11:26:44.074587Z","shell.execute_reply":"2024-12-04T11:26:44.086317Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"pos_agg = pos_cash(path)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-04T11:26:44.087929Z","iopub.execute_input":"2024-12-04T11:26:44.088181Z","iopub.status.idle":"2024-12-04T11:30:43.536595Z","shell.execute_reply.started":"2024-12-04T11:26:44.088155Z","shell.execute_reply":"2024-12-04T11:30:43.535671Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## INSTALLMENTS_PAYMENTS","metadata":{}},{"cell_type":"code","source":"def parallel_apply(groups, func, index_name='Index', num_workers=0, chunk_size=100000):\n    if num_workers <= 0:\n        num_workers = 4\n    indeces, features = [], []\n    for index_chunk, groups_chunk in chunk_groups(groups, chunk_size):\n        with mp.pool.Pool(num_workers) as executor:\n            features_chunk = executor.map(func, groups_chunk)\n        features.extend(features_chunk)\n        indeces.extend(index_chunk)\n\n    features = pd.DataFrame(features)\n    features.index = indeces\n    features.index.name = index_name\n    return features\n\ndef chunk_groups(groupby_object, chunk_size):\n    n_groups = groupby_object.ngroups\n    group_chunk, index_chunk = [], []\n    for i, (index, df) in enumerate(groupby_object):\n        group_chunk.append(df)\n        index_chunk.append(index)\n        if (i + 1) % chunk_size == 0 or i + 1 == n_groups:\n            group_chunk_, index_chunk_ = group_chunk.copy(), index_chunk.copy()\n            group_chunk, index_chunk = [], []\n            yield index_chunk_, group_chunk_\n\ndef installments_last_loan_features(gr):\n    '''\n    Calculate features for the last loan in installments_payments.csv.\n        Input:\n            gr : pandas.DataFrame\n                DataFrame groupby object.\n        Output:\n            features : dict\n                Dictionary with new features.\n    '''\n    gr_ = gr.copy()\n    gr_.sort_values(['DAYS_INSTALMENT'], ascending=False, inplace=True)\n    last_installment_id = gr_['SK_ID_PREV'].iloc[0]\n    gr_ = gr_[gr_['SK_ID_PREV'] == last_installment_id]\n\n    features = {}\n    features = add_features_in_group(features, gr_, 'DPD',\n                                     ['sum', 'mean', 'max', 'std'],\n                                     'LAST_LOAN_')\n    features = add_features_in_group(features, gr_, 'LATE_PAYMENT',\n                                     ['count', 'mean'],\n                                     'LAST_LOAN_')\n    features = add_features_in_group(features, gr_, 'PAID_OVER_AMOUNT',\n                                     ['sum', 'mean', 'max', 'min', 'std'],\n                                     'LAST_LOAN_')\n    features = add_features_in_group(features, gr_, 'PAID_OVER',\n                                     ['count', 'mean'],\n                                     'LAST_LOAN_')\n    return features","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-04T11:30:43.537763Z","iopub.execute_input":"2024-12-04T11:30:43.538014Z","iopub.status.idle":"2024-12-04T11:30:43.547648Z","shell.execute_reply.started":"2024-12-04T11:30:43.537989Z","shell.execute_reply":"2024-12-04T11:30:43.546875Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"INSTALLMENTS_AGG = {\n    'SK_ID_PREV': ['size', 'nunique'],\n\n    'AMT_INSTALMENT': ['min', 'max', 'mean', 'sum'],\n    'AMT_PAYMENT': ['min', 'max', 'mean', 'sum'],\n\n    'DAYS_ENTRY_PAYMENT': ['min', 'max', 'mean'],\n    'DPD': ['max', 'mean', 'var'],\n    'DBD': ['max', 'mean', 'var'],\n    'DPD_7': ['mean'],\n    'DPD_15': ['mean'],\n\n    'INS_IS_DPD_UNDER_120': ['mean', 'sum'],\n    'INS_IS_DPD_OVER_120': ['mean', 'sum'],\n\n    'LATE_PAYMENT': ['mean', 'sum'],\n    'LATE_PAYMENT_RATIO': ['mean'],\n\n    'PAID_OVER': ['mean'],\n    'PAYMENT_DIFFERENCE': ['mean'],\n    'PAYMENT_RATIO': ['mean'],\n\n    'SIGNIFICANT_LATE_PAYMENT': ['mean', 'sum'],\n}\n\nINSTALLMENTS_TIME_AGG = {\n    'SK_ID_PREV': ['size'],\n\n    'AMT_INSTALMENT': ['min', 'max', 'mean', 'sum'],\n    'AMT_PAYMENT': ['min', 'max', 'mean', 'sum'],\n\n    'DAYS_ENTRY_PAYMENT': ['min', 'max', 'mean'],\n    'DPD': ['max', 'mean', 'var'],\n    'DBD': ['max', 'mean', 'var'],\n    'DPD_7': ['mean'],\n    'DPD_15': ['mean'],\n\n    'LATE_PAYMENT': ['mean'],\n    'LATE_PAYMENT_RATIO': ['mean'],\n\n    'PAYMENT_DIFFERENCE': ['mean'],\n    'PAYMENT_RATIO': ['mean'],\n\n    'SIGNIFICANT_LATE_PAYMENT': ['mean'],\n}\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-04T11:30:43.548648Z","iopub.execute_input":"2024-12-04T11:30:43.548888Z","iopub.status.idle":"2024-12-04T11:30:43.561900Z","shell.execute_reply.started":"2024-12-04T11:30:43.548865Z","shell.execute_reply":"2024-12-04T11:30:43.561076Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def installment(path_to_data):\n    \"\"\" Process dseb63_installments_payments.csv and return a pandas dataframe. \"\"\"\n    # Read data\n    pay = pd.read_csv(os.path.join(\n        path_to_data, 'dseb63_installments_payments.csv'))\n\n    # Group payments and get Payment difference\n    pay = do_sum(pay, ['SK_ID_PREV', 'NUM_INSTALMENT_NUMBER'],\n                 'AMT_PAYMENT', 'AMT_PAYMENT_GROUPED')\n    pay['PAYMENT_DIFFERENCE'] = pay['AMT_INSTALMENT'] - \\\n        pay['AMT_PAYMENT_GROUPED']\n    pay['PAYMENT_RATIO'] = pay['AMT_INSTALMENT'] / pay['AMT_PAYMENT_GROUPED']\n    pay['PAID_OVER_AMOUNT'] = pay['AMT_PAYMENT'] - pay['AMT_INSTALMENT']\n    pay['PAID_OVER'] = (pay['PAID_OVER_AMOUNT'] > 0).astype(int)\n\n    # Payment Entry: Days past due and Days before due\n    pay['DPD'] = pay['DAYS_ENTRY_PAYMENT'] - pay['DAYS_INSTALMENT']\n    pay['DPD'] = pay['DPD'].apply(lambda x: 0 if x <= 0 else x)\n    pay['DBD'] = pay['DAYS_INSTALMENT'] - pay['DAYS_ENTRY_PAYMENT']\n    pay['DBD'] = pay['DBD'].apply(lambda x: 0 if x <= 0 else x)\n\n    # Flag late payment\n    pay['LATE_PAYMENT'] = pay['DBD'].apply(lambda x: 1 if x > 0 else 0)\n\n    # Percentage of payments that were late\n    pay['INSTALMENT_PAYMENT_RATIO'] = pay['AMT_PAYMENT'] / pay['AMT_INSTALMENT']\n    pay['LATE_PAYMENT_RATIO'] = pay.apply(\n        lambda x: x['INSTALMENT_PAYMENT_RATIO'] if x['LATE_PAYMENT'] == 1 else 0, axis=1)\n\n    # Flag late payments that have a significant amount\n    pay['SIGNIFICANT_LATE_PAYMENT'] = pay['LATE_PAYMENT_RATIO'].apply(\n        lambda x: 1 if x > 0.05 else 0)\n\n    # Flag k threshold late payments\n    pay['DPD_7'] = pay['DPD'].apply(lambda x: 1 if x >= 7 else 0)\n    pay['DPD_15'] = pay['DPD'].apply(lambda x: 1 if x >= 15 else 0)\n\n    # Flag k threshold late payments over or under 120 days\n    pay['INS_IS_DPD_UNDER_120'] = pay['DPD'].apply(\n        lambda x: 1 if (x > 0) & (x < 120) else 0)\n    pay['INS_IS_DPD_OVER_120'] = pay['DPD'].apply(\n        lambda x: 1 if (x >= 120) else 0)\n\n    # Aggregations by SK_ID_CURR\n    pay_agg = group(pay, 'INS_', INSTALLMENTS_AGG)\n\n    # Installments in the last x months\n    for months in [24, 60]:\n        recent_prev_id = pay[pay['DAYS_INSTALMENT']\n                             >= -30*months]['SK_ID_PREV'].unique()\n        pay_recent = pay[pay['SK_ID_PREV'].isin(recent_prev_id)]\n        prefix = f'INS_{months}M_'\n        pay_agg = group_and_merge(\n            pay_recent, pay_agg, prefix, INSTALLMENTS_TIME_AGG)\n\n    # Last loan features\n    group_features = ['SK_ID_CURR', 'SK_ID_PREV', 'DPD', 'LATE_PAYMENT',\n                      'PAID_OVER_AMOUNT', 'PAID_OVER', 'DAYS_INSTALMENT']\n    gp = pay[group_features].groupby('SK_ID_CURR')\n    del pay\n    g = parallel_apply(gp, installments_last_loan_features,\n                       index_name='SK_ID_CURR', chunk_size=10000).reset_index()\n    pay_agg = pay_agg.merge(g, on='SK_ID_CURR', how='left')\n\n    del g, gp\n    gc.collect()\n    return pay_agg\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-04T11:30:43.564023Z","iopub.execute_input":"2024-12-04T11:30:43.564459Z","iopub.status.idle":"2024-12-04T11:30:43.575403Z","shell.execute_reply.started":"2024-12-04T11:30:43.564433Z","shell.execute_reply":"2024-12-04T11:30:43.574613Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"install_pay = installment(path)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-04T11:30:43.576329Z","iopub.execute_input":"2024-12-04T11:30:43.576579Z","iopub.status.idle":"2024-12-04T11:34:03.525592Z","shell.execute_reply.started":"2024-12-04T11:30:43.576553Z","shell.execute_reply":"2024-12-04T11:34:03.524210Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## CREDIT_CARD_BALANCE","metadata":{}},{"cell_type":"code","source":"CREDIT_CARD_AGG = {\n    'AMT_BALANCE': ['sum', 'min', 'max'],\n    'AMT_CREDIT_LIMIT_ACTUAL': ['max', 'sum', 'min'],\n    'AMT_DRAWINGS_ATM_CURRENT': ['max', 'sum'],\n    'AMT_DRAWINGS_CURRENT': ['max', 'sum'],\n    'AMT_DRAWINGS_POS_CURRENT': ['max', 'sum'],\n    'AMT_DRAWING_SUM': ['sum', 'max'],\n    'AMT_INST_MIN_REGULARITY': ['max', 'mean', 'min'],\n    'AMT_INTEREST_RECEIVABLE': ['min', 'mean'],\n    'AMT_PAYMENT_TOTAL_CURRENT': ['max', 'mean', 'sum', 'var'],\n    'AMT_TOTAL_RECEIVABLE': ['max', 'mean', 'sum'],\n\n    'BALANCE_LIMIT_RATIO': ['mean', 'max', 'min'],\n\n    'CNT_DRAWING_SUM': ['sum', 'max'],\n    'CNT_DRAWINGS_ATM_CURRENT': ['max', 'mean', 'sum'],\n    'CNT_DRAWINGS_CURRENT': ['max', 'mean', 'sum'],\n    'CNT_DRAWINGS_POS_CURRENT': ['mean', 'sum', 'max'],\n\n    'EXP_AMT_BALANCE': ['last'],\n    'EXP_AMT_CREDIT_LIMIT_ACTUAL': ['last'],\n    'EXP_AMT_RECEIVABLE_PRINCIPAL': ['last'],\n    'EXP_AMT_RECEIVABLE': ['last'],\n    'EXP_AMT_TOTAL_RECEIVABLE': ['last'],\n    'EXP_AMT_DRAWING_SUM': ['last'],\n    'EXP_BALANCE_LIMIT_RATIO': ['last'],\n    'EXP_CNT_DRAWING_SUM': ['last'],\n    'EXP_MIN_PAYMENT_RATIO': ['last'],\n    'EXP_PAYMENT_MIN_DIFF': ['last'],\n    'EXP_MIN_PAYMENT_TOTAL_RATIO': ['last'],\n    'EXP_AMT_INTEREST_RECEIVABLE': ['last'],\n    'EXP_SK_DPD_RATIO': ['last'],\n\n    'LIMIT_USE': ['max', 'mean'],\n    'LATE_PAYMENT': ['max', 'sum'],\n\n    'MIN_PAYMENT_RATIO': ['min', 'mean'],\n    'MIN_PAYMENT_TOTAL_RATIO': ['min', 'mean'],\n    'MONTHS_BALANCE': ['min'],\n\n    'SK_DPD': ['mean', 'max', 'sum'],\n    'SK_DPD_DEF': ['max', 'sum'],\n    'SK_DPD_RATIO': ['max', 'mean'],\n\n    'PAYMENT_DIV_MIN': ['min', 'mean'],\n    'PAYMENT_MIN_DIFF': ['min', 'mean'],\n    'PAYMENT_MIN_TOTAL_DIFF': ['min', 'mean'],\n}\n\nCREDIT_CARD_TIME_AGG = {\n    'AMT_BALANCE': ['mean', 'max'],\n\n    'CNT_DRAWINGS_ATM_CURRENT': ['mean'],\n\n    'EXP_AMT_DRAWING_SUM': ['last'],\n    'EXP_BALANCE_LIMIT_RATIO': ['last'],\n    'EXP_CNT_DRAWING_SUM': ['last'],\n    'EXP_MIN_PAYMENT_RATIO': ['last'],\n    'EXP_PAYMENT_MIN_DIFF': ['last'],\n    'EXP_MIN_PAYMENT_TOTAL_RATIO': ['last'],\n    'EXP_AMT_INTEREST_RECEIVABLE': ['last'],\n    'EXP_SK_DPD_RATIO': ['last'],\n\n    'LIMIT_USE': ['max', 'mean'],\n\n    'SK_DPD': ['max', 'sum'],\n}\n\nrolling_columns = [\n    'AMT_BALANCE',\n    'AMT_CREDIT_LIMIT_ACTUAL',\n    'AMT_DRAWING_SUM',\n    'AMT_INTEREST_RECEIVABLE',\n    'AMT_RECEIVABLE',\n    'AMT_RECEIVABLE_PRINCIPAL',\n    'AMT_TOTAL_RECEIVABLE',\n\n    'BALANCE_LIMIT_RATIO',\n\n    'CNT_DRAWING_SUM',\n\n    'MIN_PAYMENT_RATIO',\n    'MIN_PAYMENT_TOTAL_RATIO',\n\n    'PAYMENT_MIN_DIFF',\n\n    'SK_DPD_RATIO']\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-04T11:34:03.527319Z","iopub.execute_input":"2024-12-04T11:34:03.527598Z","iopub.status.idle":"2024-12-04T11:34:03.539990Z","shell.execute_reply.started":"2024-12-04T11:34:03.527571Z","shell.execute_reply":"2024-12-04T11:34:03.539225Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\ndef credit_card(path_to_data):\n    \"\"\" Process dseb63_credit_card_balance.csv and return a pandas dataframe. \"\"\"\n    # Read data\n    cc = pd.read_csv(os.path.join(\n        path_to_data, 'dseb63_credit_card_balance.csv'))\n\n    # One-hot encoder\n    cc, _ = one_hot_encoder(cc, nan_as_category=False)\n\n    # Rename columns to correct format\n    cc.rename(columns={'AMT_RECIVABLE': 'AMT_RECEIVABLE'}, inplace=True)\n\n    # Amount used from limit\n    cc['LIMIT_USE'] = cc['AMT_BALANCE'] / cc['AMT_CREDIT_LIMIT_ACTUAL']\n\n    # Current payment / Min payment\n    cc['PAYMENT_DIV_MIN'] = cc['AMT_PAYMENT_CURRENT'] / \\\n        cc['AMT_INST_MIN_REGULARITY']\n\n    # Late payment\n    cc['LATE_PAYMENT'] = cc['SK_DPD'].apply(lambda x: 1 if x > 0 else 0)\n\n    # How much drawing of limit\n    cc['DRAWING_LIMIT_RATIO'] = cc['AMT_DRAWINGS_ATM_CURRENT'] / \\\n        cc['AMT_CREDIT_LIMIT_ACTUAL']\n\n    # Total amount of drawing at ATM\n    cc['AMT_DRAWING_SUM'] = cc['AMT_DRAWINGS_ATM_CURRENT'] + cc['AMT_DRAWINGS_CURRENT'] + cc[\n        'AMT_DRAWINGS_OTHER_CURRENT'] + cc['AMT_DRAWINGS_POS_CURRENT']\n\n    # Total of drawing on previous credit\n    cc['CNT_DRAWING_SUM'] = cc['CNT_DRAWINGS_ATM_CURRENT'] + cc['CNT_DRAWINGS_CURRENT'] + \\\n        cc['CNT_DRAWINGS_OTHER_CURRENT'] + \\\n        cc['CNT_DRAWINGS_POS_CURRENT'] + cc['CNT_INSTALMENT_MATURE_CUM']\n\n    # ATM balance ratio, with smoothing by 0.00001 to avoid dividing by zero\n    cc['BALANCE_LIMIT_RATIO'] = cc['AMT_BALANCE'] / \\\n        (cc['AMT_CREDIT_LIMIT_ACTUAL'] + 0.00001)\n\n    # Number of times drawing was done, with smoothing by 0.00001 to avoid dividing by zero\n    cc['MIN_PAYMENT_RATIO'] = cc['AMT_PAYMENT_CURRENT'] / \\\n        (cc['AMT_INST_MIN_REGULARITY'] + 0.0001)\n    cc['MIN_PAYMENT_TOTAL_RATIO'] = cc['AMT_PAYMENT_TOTAL_CURRENT'] / \\\n        (cc['AMT_INST_MIN_REGULARITY'] + 0.00001)\n\n    # Days past due ratio, with smoothing by 0.00001 to avoid dividing by zero\n    cc['SK_DPD_RATIO'] = cc['SK_DPD'] / (cc['SK_DPD_DEF'] + 0.00001)\n\n    # Difference in payment min and current\n    cc['PAYMENT_MIN_DIFF'] = cc['AMT_PAYMENT_CURRENT'] - \\\n        cc['AMT_INST_MIN_REGULARITY']\n\n    # Difference in total payment and current\n    cc['PAYMENT_MIN_TOTAL_DIFF'] = cc['AMT_PAYMENT_TOTAL_CURRENT'] - \\\n        cc['AMT_INST_MIN_REGULARITY']\n\n    # Interest received\n    cc['AMT_INTEREST_RECEIVABLE'] = cc['AMT_TOTAL_RECEIVABLE'] - \\\n        cc['AMT_RECEIVABLE_PRINCIPAL']\n\n    # calculating the rolling Exponential Weighted Moving Average over months for certain features\n    exp_weighted_columns = ['EXP_' + ele for ele in rolling_columns]\n    cc[exp_weighted_columns] = cc.groupby(['SK_ID_CURR', 'SK_ID_PREV'])[\n        rolling_columns].transform(lambda x: x.ewm(alpha=0.7).mean())\n\n    # Aggregations by SK_ID_CURR\n    cc_agg = cc.groupby('SK_ID_CURR').agg(CREDIT_CARD_AGG)\n    cc_agg.columns = pd.Index(['CC_' + e[0] + \"_\" + e[1].upper()\n                              for e in cc_agg.columns.tolist()])\n    cc_agg.reset_index(inplace=True)\n\n    # Last month balance of each credit card application\n    last_ids = cc.groupby('SK_ID_PREV')['MONTHS_BALANCE'].idxmax()\n    last_months_df = cc[cc.index.isin(last_ids)]\n    cc_agg = group_and_merge(last_months_df, cc_agg, 'CC_LAST_', {\n                             'AMT_BALANCE': ['mean', 'max']})\n\n    # Aggregations for last x months\n    for months in [12, 24, 48]:\n        cc_prev_id = cc[cc['MONTHS_BALANCE'] >= -months]['SK_ID_PREV'].unique()\n        cc_recent = cc[cc['SK_ID_PREV'].isin(cc_prev_id)]\n        prefix = f'INS_{months}M_'\n        cc_agg = group_and_merge(\n            cc_recent, cc_agg, prefix, CREDIT_CARD_TIME_AGG)\n\n    del cc_recent, cc_prev_id, last_months_df, last_ids, cc\n    gc.collect()\n    return cc_agg\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-04T11:34:03.541150Z","iopub.execute_input":"2024-12-04T11:34:03.541425Z","iopub.status.idle":"2024-12-04T11:34:03.555837Z","shell.execute_reply.started":"2024-12-04T11:34:03.541398Z","shell.execute_reply":"2024-12-04T11:34:03.555106Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"cc = credit_card(path)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-04T11:34:03.556752Z","iopub.execute_input":"2024-12-04T11:34:03.556984Z","iopub.status.idle":"2024-12-04T11:35:11.462760Z","shell.execute_reply.started":"2024-12-04T11:34:03.556960Z","shell.execute_reply":"2024-12-04T11:35:11.462018Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# AGGREGATE ALL TABLES","metadata":{}},{"cell_type":"markdown","source":"We have merged the bureau and bureau_balance outside this notebook. The application_train and application_test were merged for encoding and feature engineering (we handle outliers and missing values of each table separately before merging).","metadata":{}},{"cell_type":"code","source":"bureau_merged=pd.read_csv('/kaggle/input/bureau-merge-full-col/bureau_merge_no_drop and full col.csv')\napplication_merged= pd.read_csv('/kaggle/input/feature-eng/FeatureEngineering.csv')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-04T11:35:11.464059Z","iopub.execute_input":"2024-12-04T11:35:11.464327Z","iopub.status.idle":"2024-12-04T11:35:30.825849Z","shell.execute_reply.started":"2024-12-04T11:35:11.464302Z","shell.execute_reply":"2024-12-04T11:35:30.824948Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Merge bureau_merge\ntrain_test_merge = pd.merge(application_merged, bureau_merged, on='SK_ID_CURR', how='left')\nprint(train_test_merge.shape)\n\n# Merge credit_card_balance\ntrain_test_merge = pd.merge(train_test_merge, cc, on='SK_ID_CURR', how='left')\nprint(train_test_merge.shape)\n\n# Merge pos\ntrain_test_merge = pd.merge(train_test_merge, pos_agg, on='SK_ID_CURR', how='left')\nprint(train_test_merge.shape)\n\n# Merge prev_app_agg\ntrain_test_merge = pd.merge(train_test_merge, prev, on='SK_ID_CURR', how='left')\nprint(train_test_merge.shape)\n\n# Merge install_pay_agg\ntrain_test_merge = pd.merge(train_test_merge, install_pay, on='SK_ID_CURR', how='left')\ntrain_test_merge.shape","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-04T11:35:30.827151Z","iopub.execute_input":"2024-12-04T11:35:30.827477Z","iopub.status.idle":"2024-12-04T11:35:39.367284Z","shell.execute_reply.started":"2024-12-04T11:35:30.827446Z","shell.execute_reply":"2024-12-04T11:35:39.366448Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_test_merge.drop('Unnamed: 0',axis= 1, inplace= True)\ntrain_test_merge.shape","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-04T11:35:39.368723Z","iopub.execute_input":"2024-12-04T11:35:39.369025Z","iopub.status.idle":"2024-12-04T11:35:39.985085Z","shell.execute_reply.started":"2024-12-04T11:35:39.368997Z","shell.execute_reply":"2024-12-04T11:35:39.984320Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# CHECK MISSING VALUES OF MERGED DATASET","metadata":{}},{"cell_type":"code","source":"def check_nan(col):\n    num_nan = col.isnull().sum()\n    num_per = num_nan/len(col) * 100\n    col_miss = pd.concat([num_nan, num_per], axis=1, keys=['number_of_NaN', 'percentage_of_NaN']).sort_values(by='percentage_of_NaN', ascending = False).round(1)\n    col_miss = col_miss[col_miss['number_of_NaN'] != 0]\n    print('Num fields: ', col.shape[1])\n    print('Num missing fields: ', col_miss.shape[0])\n    return col_miss\ncheck_nan(train_test_merge)[:40]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-04T11:35:39.986152Z","iopub.execute_input":"2024-12-04T11:35:39.986413Z","iopub.status.idle":"2024-12-04T11:35:40.380498Z","shell.execute_reply.started":"2024-12-04T11:35:39.986389Z","shell.execute_reply":"2024-12-04T11:35:40.379603Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train=train_test_merge.head(246009)\ntest=train_test_merge.tail(61502)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-04T11:35:40.381650Z","iopub.execute_input":"2024-12-04T11:35:40.381966Z","iopub.status.idle":"2024-12-04T11:35:40.385894Z","shell.execute_reply.started":"2024-12-04T11:35:40.381935Z","shell.execute_reply":"2024-12-04T11:35:40.385147Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#train.to_csv('train_original.csv',index=False)\n#test.to_csv('test_original.csv', index=False)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-29T18:19:59.988689Z","iopub.execute_input":"2024-11-29T18:19:59.989181Z","iopub.status.idle":"2024-11-29T18:23:40.181582Z","shell.execute_reply.started":"2024-11-29T18:19:59.989146Z","shell.execute_reply":"2024-11-29T18:23:40.180127Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"check_nan(train)[:50]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-03T04:30:20.733438Z","iopub.execute_input":"2024-12-03T04:30:20.733896Z","iopub.status.idle":"2024-12-03T04:30:21.048433Z","shell.execute_reply.started":"2024-12-03T04:30:20.733858Z","shell.execute_reply":"2024-12-03T04:30:21.047752Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"check_nan(test)[:60]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-03T04:31:24.693294Z","iopub.execute_input":"2024-12-03T04:31:24.693971Z","iopub.status.idle":"2024-12-03T04:31:24.776997Z","shell.execute_reply.started":"2024-12-03T04:31:24.693936Z","shell.execute_reply":"2024-12-03T04:31:24.776271Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"high_null_col= train.columns[train.isnull().sum()/246009 >= 0.9]\ntrain_drop_highna=train.drop(columns= high_null_col)\ntrain_drop_highna.shape","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-04T11:35:40.389101Z","iopub.execute_input":"2024-12-04T11:35:40.389406Z","iopub.status.idle":"2024-12-04T11:35:41.160244Z","shell.execute_reply.started":"2024-12-04T11:35:40.389379Z","shell.execute_reply":"2024-12-04T11:35:41.159379Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"check_nan(test[high_null_col])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-03T04:30:58.811948Z","iopub.execute_input":"2024-12-03T04:30:58.812280Z","iopub.status.idle":"2024-12-03T04:30:58.833173Z","shell.execute_reply.started":"2024-12-03T04:30:58.812252Z","shell.execute_reply":"2024-12-03T04:30:58.832528Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"test_drop_highna=test.drop(columns=high_null_col)\ncheck_nan(test_drop_highna)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-04T11:35:41.161281Z","iopub.execute_input":"2024-12-04T11:35:41.161536Z","iopub.status.idle":"2024-12-04T11:35:41.357348Z","shell.execute_reply.started":"2024-12-04T11:35:41.161512Z","shell.execute_reply":"2024-12-04T11:35:41.356521Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"We dropped high missing values ratio columns (>90%). I had to check whether these columns in train dataset are the same with those in the test dataset. They are the same but the percentages of them were in different order.","metadata":{}},{"cell_type":"markdown","source":"# DEAL WITH INFINITE VALUE","metadata":{}},{"cell_type":"code","source":"train_drop_highna.info()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-03T04:32:14.319107Z","iopub.execute_input":"2024-12-03T04:32:14.320117Z","iopub.status.idle":"2024-12-03T04:32:14.349235Z","shell.execute_reply.started":"2024-12-03T04:32:14.320083Z","shell.execute_reply":"2024-12-03T04:32:14.348466Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"test_drop_highna.info()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-03T04:32:17.029426Z","iopub.execute_input":"2024-12-03T04:32:17.029774Z","iopub.status.idle":"2024-12-03T04:32:17.057398Z","shell.execute_reply.started":"2024-12-03T04:32:17.029743Z","shell.execute_reply":"2024-12-03T04:32:17.056745Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"inf_col=train_drop_highna.columns[np.isinf(train_drop_highna).any()]\ninf_col","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-04T11:35:41.358599Z","iopub.execute_input":"2024-12-04T11:35:41.358942Z","iopub.status.idle":"2024-12-04T11:35:41.562418Z","shell.execute_reply.started":"2024-12-04T11:35:41.358909Z","shell.execute_reply":"2024-12-04T11:35:41.561619Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"inf_tcol=test_drop_highna.columns[np.isinf(test_drop_highna).any()]\ninf_tcol","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-04T11:35:41.563513Z","iopub.execute_input":"2024-12-04T11:35:41.563811Z","iopub.status.idle":"2024-12-04T11:35:41.614203Z","shell.execute_reply.started":"2024-12-04T11:35:41.563783Z","shell.execute_reply":"2024-12-04T11:35:41.613463Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_noinf = train_drop_highna.copy()\n\nfor col in inf_col.tolist():\n    max_noinf=train_noinf[col].replace(np.inf, np.nan).max()\n    min_noinf=train_noinf[col].replace(-np.inf, np.nan).min()\n    train_noinf[col] = train_noinf[col].replace([np.inf, -np.inf], \n                                                [max_noinf, min_noinf])\n\ninf_columns = train_noinf.columns[train_noinf.apply(lambda x: np.isinf(x).any())]\nprint(\"Cột còn chứa inf:\", inf_columns)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-04T11:35:41.615365Z","iopub.execute_input":"2024-12-04T11:35:41.615653Z","iopub.status.idle":"2024-12-04T11:35:43.721198Z","shell.execute_reply.started":"2024-12-04T11:35:41.615625Z","shell.execute_reply":"2024-12-04T11:35:43.720352Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"test_noinf = test_drop_highna.copy()\n\nfor col in inf_tcol.tolist():\n    max_noinf=test_noinf[col].replace(np.inf, np.nan).max()\n    min_noinf=test_noinf[col].replace(-np.inf, np.nan).min()\n    test_noinf[col] = test_noinf[col].replace([np.inf, -np.inf], \n                                                [max_noinf, min_noinf])\n\ninf_tcolumns = test_noinf.columns[test_noinf.apply(lambda x: np.isinf(x).any())]\nprint(\"Cột còn chứa inf:\", inf_tcolumns)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-04T11:35:43.722249Z","iopub.execute_input":"2024-12-04T11:35:43.722509Z","iopub.status.idle":"2024-12-04T11:35:44.361239Z","shell.execute_reply.started":"2024-12-04T11:35:43.722482Z","shell.execute_reply":"2024-12-04T11:35:44.360359Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"test_noinf.select_dtypes(include='int64').nunique()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-03T04:32:46.028242Z","iopub.execute_input":"2024-12-03T04:32:46.029054Z","iopub.status.idle":"2024-12-03T04:32:46.040119Z","shell.execute_reply.started":"2024-12-03T04:32:46.029020Z","shell.execute_reply":"2024-12-03T04:32:46.039450Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#drop columns with only 1 unique value\ntrain_noinf.drop(columns=['AGE_RANGE','RETIREMENT_AGE'], inplace=True)\ntest_noinf.drop(columns=['AGE_RANGE','RETIREMENT_AGE'], inplace=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-04T11:35:44.362365Z","iopub.execute_input":"2024-12-04T11:35:44.362645Z","iopub.status.idle":"2024-12-04T11:35:44.955450Z","shell.execute_reply.started":"2024-12-04T11:35:44.362619Z","shell.execute_reply":"2024-12-04T11:35:44.954410Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# CHECK & HANDLE OUTLIERS","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\n\ndef outlier_summary(df):\n    \"\"\"\n    Hàm kiểm tra outliers trong DataFrame.\n    Trả về bảng thống kê tương tự method `describe()`, \n    bao gồm thông tin về outliers theo 3-Sigma và IQR.\n    \n    Parameters:\n        df (pd.DataFrame): DataFrame chứa dữ liệu cần kiểm tra.\n        \n    Returns:\n        pd.DataFrame: Bảng thống kê với thông tin về outliers.\n    \"\"\"\n    summary = []\n    for col in df.select_dtypes(include=[np.number]).columns:\n        # Tính toán cơ bản\n        col_data = df[col].dropna()\n        mean = col_data.mean()\n        std = col_data.std()\n        q1 = col_data.quantile(0.25)\n        q3 = col_data.quantile(0.75)\n        iqr = q3 - q1\n\n        # Ngưỡng cho 3-Sigma\n        sigma_lower = mean - 3 * std\n        sigma_upper = mean + 3 * std\n\n        # Ngưỡng cho IQR\n        iqr_lower = q1 - 1.5 * iqr\n        iqr_upper = q3 + 1.5 * iqr\n\n        # Số lượng outliers\n        outliers_sigma = ((col_data < sigma_lower) | (col_data > sigma_upper)).sum()\n        outliers_iqr = ((col_data < iqr_lower) | (col_data > iqr_upper)).sum()\n\n        # Tạo hàng thống kê\n        summary.append({\n            \"Feature\": col,\n            #\"Count\": len(col_data),\n            #\"Mean\": mean,\n            #\"Std\": std,\n            \"Min\": col_data.min(),\n            \"Q1\": q1,\n            \"Median\": col_data.median(),\n            \"Q3\": q3,\n            \"Max\": col_data.max(),\n            \"Outliers (3-Sigma)\": outliers_sigma,\n            \"Outliers (IQR)\": outliers_iqr\n        })\n    \n    return pd.DataFrame(summary).set_index(\"Feature\")\n\n\nresult = outlier_summary(train_noinf)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(result[result['Outliers (IQR)'] !=0])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-04T12:07:33.648477Z","iopub.execute_input":"2024-12-04T12:07:33.649284Z","iopub.status.idle":"2024-12-04T12:07:33.658341Z","shell.execute_reply.started":"2024-12-04T12:07:33.649243Z","shell.execute_reply":"2024-12-04T12:07:33.657746Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"result['Outliers (IQR)'][result['Outliers (IQR)']>0].sort_values(ascending=False)[:100]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-04T12:09:24.791657Z","iopub.execute_input":"2024-12-04T12:09:24.792366Z","iopub.status.idle":"2024-12-04T12:09:24.800820Z","shell.execute_reply.started":"2024-12-04T12:09:24.792330Z","shell.execute_reply":"2024-12-04T12:09:24.800045Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def sigma3_method(series):\n    mean = series.mean()\n    std_dev = series.std()\n    lower_bound = mean - 3 * std_dev\n    upper_bound = mean + 3 * std_dev\n    return series.clip(lower_bound, upper_bound)\n\ndef impute_missing_values(df):\n    for col in df.columns:\n        if df[col].nunique() < 100:  # Categorical column\n            df[col] = df[col].fillna(df[col].mode()[0])\n        else:  # Numerical column\n            df[col] = sigma3_method(df[col])\n            df[col] = df[col].fillna(df[col].mean())\n    return df","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-04T12:47:26.256222Z","iopub.execute_input":"2024-12-04T12:47:26.256941Z","iopub.status.idle":"2024-12-04T12:47:26.262798Z","shell.execute_reply.started":"2024-12-04T12:47:26.256904Z","shell.execute_reply":"2024-12-04T12:47:26.261990Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_fill=impute_missing_values(train_noinf)\ncheck_nan(train_fill)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-04T12:47:49.166986Z","iopub.execute_input":"2024-12-04T12:47:49.167344Z","iopub.status.idle":"2024-12-04T12:48:00.420493Z","shell.execute_reply.started":"2024-12-04T12:47:49.167313Z","shell.execute_reply":"2024-12-04T12:48:00.419656Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"test_fill=impute_missing_values(test_noinf)\ncheck_nan(test_fill)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-04T12:49:22.047858Z","iopub.execute_input":"2024-12-04T12:49:22.048525Z","iopub.status.idle":"2024-12-04T12:49:24.662229Z","shell.execute_reply.started":"2024-12-04T12:49:22.048487Z","shell.execute_reply":"2024-12-04T12:49:24.661442Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_fill.drop('SK_ID_CURR', axis=1, inplace=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-04T12:52:42.331860Z","iopub.execute_input":"2024-12-04T12:52:42.332785Z","iopub.status.idle":"2024-12-04T12:52:42.991512Z","shell.execute_reply.started":"2024-12-04T12:52:42.332748Z","shell.execute_reply":"2024-12-04T12:52:42.990753Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_fill.shape","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-04T12:52:53.271624Z","iopub.execute_input":"2024-12-04T12:52:53.272299Z","iopub.status.idle":"2024-12-04T12:52:53.277294Z","shell.execute_reply.started":"2024-12-04T12:52:53.272265Z","shell.execute_reply":"2024-12-04T12:52:53.276565Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#train_fill.to_csv('train.csv', index=False)\n#test_fill.to_csv('test.csv', index=False)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-04T12:25:45.531177Z","iopub.execute_input":"2024-12-04T12:25:45.531529Z","iopub.status.idle":"2024-12-04T12:32:41.197468Z","shell.execute_reply.started":"2024-12-04T12:25:45.531499Z","shell.execute_reply":"2024-12-04T12:32:41.196363Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# CORRELATION","metadata":{}},{"cell_type":"code","source":"target= pd.read_csv('/kaggle/input/final-dataset/target.csv')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-04T12:37:18.734359Z","iopub.execute_input":"2024-12-04T12:37:18.734855Z","iopub.status.idle":"2024-12-04T12:37:18.769129Z","shell.execute_reply.started":"2024-12-04T12:37:18.734813Z","shell.execute_reply":"2024-12-04T12:37:18.768430Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_target=pd.concat([train_fill, target], axis=1)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-03T05:28:48.231988Z","iopub.execute_input":"2024-12-03T05:28:48.232284Z","iopub.status.idle":"2024-12-03T05:28:48.775572Z","shell.execute_reply.started":"2024-12-03T05:28:48.232257Z","shell.execute_reply":"2024-12-03T05:28:48.774631Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"correlations = train_fill.corr()['TARGET'].drop('TARGET')  # Tính tương quan của tất cả cột với 'target'","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-03T05:29:27.176852Z","iopub.execute_input":"2024-12-03T05:29:27.177274Z","iopub.status.idle":"2024-12-03T05:35:45.969040Z","shell.execute_reply.started":"2024-12-03T05:29:27.177241Z","shell.execute_reply":"2024-12-03T05:35:45.967947Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"top20_positive=correlations.sort_values(ascending=False)[:25]\ntop20_positive.plot(kind='barh')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-03T05:56:46.094156Z","iopub.execute_input":"2024-12-03T05:56:46.095233Z","iopub.status.idle":"2024-12-03T05:56:46.490869Z","shell.execute_reply.started":"2024-12-03T05:56:46.095190Z","shell.execute_reply":"2024-12-03T05:56:46.490131Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"top20_negative=correlations.sort_values(ascending=True)[:25]\ntop20_negative.plot(kind='barh', color='lightblue')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-03T05:58:23.977799Z","iopub.execute_input":"2024-12-03T05:58:23.978632Z","iopub.status.idle":"2024-12-03T05:58:24.370655Z","shell.execute_reply.started":"2024-12-03T05:58:23.978591Z","shell.execute_reply":"2024-12-03T05:58:24.369944Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Columns transformed from DAYS_BIRTH, EXT_SOURCE_1, EXT_SOURCE_2, EXT_SOURCE_1 are highly correlated to TARGET.","metadata":{}},{"cell_type":"code","source":"correlation_matrix = train_target.corr()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-03T06:01:06.066810Z","iopub.execute_input":"2024-12-03T06:01:06.067676Z","iopub.status.idle":"2024-12-03T06:07:24.438580Z","shell.execute_reply.started":"2024-12-03T06:01:06.067635Z","shell.execute_reply":"2024-12-03T06:07:24.437430Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"correlation_matrix","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-03T06:10:23.731091Z","iopub.execute_input":"2024-12-03T06:10:23.731758Z","iopub.status.idle":"2024-12-03T06:10:23.833449Z","shell.execute_reply.started":"2024-12-03T06:10:23.731699Z","shell.execute_reply":"2024-12-03T06:10:23.832601Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"target_corr = correlation_matrix['TARGET'].abs().sort_values(ascending=False)  # Thay 'TARGET' bằng cột mục tiêu của bạn\n\n# Lấy tên 25 biến có tương quan cao nhất\ntop_25_features = target_corr.head(25).index\n\n# Tạo ma trận tương quan cho 25 biến này\ntop_25_corr_matrix = train_target[top_25_features].corr()\n\n# Vẽ heatmap\nplt.figure(figsize=(12, 10))\nsns.heatmap(top_25_corr_matrix, annot=True, fmt='.2f', cmap='coolwarm', vmin=-1, vmax=1)\nplt.title(\"Correlation Heatmap of Top 25 Features\")\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-03T06:11:50.655088Z","iopub.execute_input":"2024-12-03T06:11:50.656138Z","iopub.status.idle":"2024-12-03T06:11:52.361006Z","shell.execute_reply.started":"2024-12-03T06:11:50.656083Z","shell.execute_reply":"2024-12-03T06:11:52.360241Z"}},"outputs":[],"execution_count":null}]}